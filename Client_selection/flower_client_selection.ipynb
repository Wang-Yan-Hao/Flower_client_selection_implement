{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UKlz2A5VMY7S"
      },
      "outputs": [],
      "source": [
        "%pip install -q flwr[simulation] torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWEVF_AYNuS1",
        "outputId": "59d01fe0-f807-4564-cd27-11cfb09808d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu using PyTorch 1.12.1+cu113 and Flower 1.1.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVec7BjjbEWs",
        "outputId": "53c18741-becc-4b2d-f86d-a3cb2513f36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_CLIENTS = 10\n",
        "\n",
        "def load_datasets(num_clients: int):\n",
        "    # Download and transform CIFAR-10 (train and test)\n",
        "    transform = transforms.Compose(\n",
        "      [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "    )\n",
        "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
        "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
        "    partition_size = len(trainset) // num_clients\n",
        "    lengths = [partition_size] * num_clients\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = len(ds) // 10  # 10 % validation set\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=32, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=32))\n",
        "    testloader = DataLoader(testset, batch_size=32)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vUoME5082vAP"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(net, trainloader, epochs: int):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(net(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HzT55_Sj25AL"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, net, trainloader, valloader):\n",
        "        self.cid = cid\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        print(f\"[Client {self.cid}] get_parameters\")\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        train(self.net, self.trainloader, epochs=1)\n",
        "        return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
        "    \n",
        "    def get_properties(self, config):\n",
        "        Client_RAM = {\"RAM\": 2} # Client will has there way to get RAM number, In this example we use 2GB\n",
        "        return Client_RAM\n",
        "\n",
        "\n",
        "def client_fn(cid) -> FlowerClient:\n",
        "    net = Net().to(DEVICE)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "    return FlowerClient(cid, net, trainloader, valloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QJxdTxtw4Ds_"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Union\n",
        "\n",
        "from flwr.common import (\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitIns,\n",
        "    FitRes,\n",
        "    MetricsAggregationFn,\n",
        "    NDArrays,\n",
        "    Parameters,\n",
        "    Scalar,\n",
        "    ndarrays_to_parameters,\n",
        "    parameters_to_ndarrays,\n",
        "    GetPropertiesIns\n",
        ")\n",
        "from flwr.server.client_manager import ClientManager\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "\n",
        "class Client_selection_RAM_strategy(fl.server.strategy.FedAvg):\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Client_selection_RAM_strategy\"\n",
        "\n",
        "    def configure_fit(\n",
        "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
        "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
        "        \"\"\"Configure the next round of training.\"\"\"\n",
        "        weights = parameters_to_ndarrays(parameters)\n",
        "        self.pre_weights = weights\n",
        "        parameters = ndarrays_to_parameters(weights)\n",
        "        config = {}\n",
        "        if self.on_fit_config_fn is not None:\n",
        "            # Custom fit config function provided\n",
        "            config = self.on_fit_config_fn(server_round)\n",
        "        fit_ins = FitIns(parameters, config)\n",
        "\n",
        "        # Sample clients\n",
        "        sample_size, min_num_clients = self.num_fit_clients(\n",
        "            client_manager.num_available()\n",
        "        )\n",
        "        \n",
        "        all_clients = client_manager.all() # Dict[str, ClientProxy], Return all available clients\n",
        "        selected_clients = [] # the clients list we will select\n",
        "\n",
        "        for client in all_clients.values(): # look all clients  \n",
        "            config_properties = GetPropertiesIns({\"RAM\": 0}) # config (Config) – Configuration parameters requested by the server. This can be used to tell the client which properties are needed along with some Scalar attributes.\n",
        "            client_properties = client.get_properties(config_properties, timeout=2.0) # get each client properties, pass GetPropertiesIns and timeout parameter\n",
        "            client_propertie = client_properties.properties\n",
        "            if client_propertie[\"RAM\"] > 1: # Choose the client which RAM > 1GB\n",
        "                selected_clients.append(client)\n",
        "\n",
        "        if len(selected_clients) == 0:\n",
        "            print(\"No client has be selected\")\n",
        "        # Return client/config pairs\n",
        "        return [(client, fit_ins) for client in selected_clients]      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2w8C2x_3NgQ",
        "outputId": "7a24e3d6-891b-420a-9877-85d22f963d7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flower 2022-11-20 13:21:53,846 | app.py:143 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
            "INFO:flower:Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
            "2022-11-20 13:21:55,551\tINFO worker.py:1518 -- Started a local Ray instance.\n",
            "INFO flower 2022-11-20 13:21:58,455 | app.py:177 | Flower VCE: Ray initialized with resources: {'memory': 7956017972.0, 'CPU': 2.0, 'node:172.28.0.2': 1.0, 'object_store_memory': 3978008985.0}\n",
            "INFO:flower:Flower VCE: Ray initialized with resources: {'memory': 7956017972.0, 'CPU': 2.0, 'node:172.28.0.2': 1.0, 'object_store_memory': 3978008985.0}\n",
            "INFO flower 2022-11-20 13:21:58,463 | server.py:86 | Initializing global parameters\n",
            "INFO:flower:Initializing global parameters\n",
            "INFO flower 2022-11-20 13:21:58,467 | server.py:270 | Requesting initial parameters from one random client\n",
            "INFO:flower:Requesting initial parameters from one random client\n",
            "INFO flower 2022-11-20 13:22:03,043 | server.py:274 | Received initial parameters from one random client\n",
            "INFO:flower:Received initial parameters from one random client\n",
            "INFO flower 2022-11-20 13:22:03,066 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flower:Evaluating initial parameters\n",
            "INFO flower 2022-11-20 13:22:03,083 | server.py:101 | FL starting\n",
            "INFO:flower:FL starting\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=4740)\u001b[0m [Client 1] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flower 2022-11-20 13:22:08,940 | server.py:220 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flower:fit_round 1: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=4740)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=4741)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=4740)\u001b[0m Epoch 1: train loss 0.06344978511333466, accuracy 0.24022222222222223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flower 2022-11-20 13:22:18,425 | server.py:234 | fit_round 1 received 2 results and 0 failures\n",
            "DEBUG:flower:fit_round 1 received 2 results and 0 failures\n",
            "WARNING flower 2022-11-20 13:22:18,451 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flower:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flower 2022-11-20 13:22:18,461 | server.py:170 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flower:evaluate_round 1: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=4741)\u001b[0m Epoch 1: train loss 0.06556787341833115, accuracy 0.2262222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=4740)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=4741)\u001b[0m [Client 1] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flower 2022-11-20 13:22:21,765 | server.py:184 | evaluate_round 1 received 2 results and 0 failures\n",
            "DEBUG:flower:evaluate_round 1 received 2 results and 0 failures\n",
            "WARNING flower 2022-11-20 13:22:21,770 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flower:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flower 2022-11-20 13:22:24,597 | server.py:220 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flower:fit_round 2: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=4741)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=4740)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=4741)\u001b[0m Epoch 1: train loss 0.05627148225903511, accuracy 0.3388888888888889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flower 2022-11-20 13:22:32,780 | server.py:234 | fit_round 2 received 2 results and 0 failures\n",
            "DEBUG:flower:fit_round 2 received 2 results and 0 failures\n",
            "DEBUG flower 2022-11-20 13:22:32,793 | server.py:170 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flower:evaluate_round 2: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=4740)\u001b[0m Epoch 1: train loss 0.05621957778930664, accuracy 0.3388888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=4740)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=4741)\u001b[0m [Client 1] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flower 2022-11-20 13:22:36,067 | server.py:184 | evaluate_round 2 received 2 results and 0 failures\n",
            "DEBUG:flower:evaluate_round 2 received 2 results and 0 failures\n",
            "INFO flower 2022-11-20 13:22:36,075 | server.py:144 | FL finished in 32.97151034700073\n",
            "INFO:flower:FL finished in 32.97151034700073\n",
            "INFO flower 2022-11-20 13:22:36,080 | app.py:192 | app_fit: losses_distributed [(1, 0.06254182982444763), (2, 0.055628442406654356)]\n",
            "INFO:flower:app_fit: losses_distributed [(1, 0.06254182982444763), (2, 0.055628442406654356)]\n",
            "INFO flower 2022-11-20 13:22:36,088 | app.py:193 | app_fit: metrics_distributed {}\n",
            "INFO:flower:app_fit: metrics_distributed {}\n",
            "INFO flower 2022-11-20 13:22:36,095 | app.py:194 | app_fit: losses_centralized []\n",
            "INFO:flower:app_fit: losses_centralized []\n",
            "INFO flower 2022-11-20 13:22:36,101 | app.py:195 | app_fit: metrics_centralized {}\n",
            "INFO:flower:app_fit: metrics_centralized {}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "History (loss, distributed):\n",
              "\tround 1: 0.06254182982444763\n",
              "\tround 2: 0.055628442406654356"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=2,\n",
        "    config=fl.server.ServerConfig(num_rounds=2),\n",
        "    strategy=Client_selection_RAM_strategy(),  # <-- pass the new strategy here\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "6415aafcaf5eb7c36aadf5c4ad6591714659a18d9c96eec418270275463d422c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
